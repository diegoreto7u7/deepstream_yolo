# ğŸ—ï¸ Arquitectura - Auto-Build TensorRT Engine

## DescripciÃ³n General

El sistema implementa una **arquitectura de auto-detecciÃ³n y construcciÃ³n de engine TensorRT** que permite ejecutar la aplicaciÃ³n en cualquier PC con GPU NVIDIA, generando automÃ¡ticamente un engine optimizado para ese hardware especÃ­fico.

## ğŸ¯ Problema que Resuelve

**Problema Original:**
- Los engines TensorRT son especÃ­ficos para cada GPU
- No puedes usar un engine generado en una RTX 4090 en una RTX 3060
- Distribuir una aplicaciÃ³n con pre-built engines es inflexible

**SoluciÃ³n:**
- El sistema detecta automÃ¡ticamente el hardware
- Genera un engine optimizado en tiempo de ejecuciÃ³n
- Compatible con cualquier GPU NVIDIA

## ğŸ“¦ Componentes Clave

### 1. **auto_build_engine.py** - Script Principal

```python
export_dynamic_batch/auto_build_engine.py
```

**Funcionalidad:**
- Detecta GPU, CUDA, TensorRT, DeepStream
- Exporta modelo YOLO PT â†’ ONNX (si es necesario)
- Construye engine TensorRT optimizado
- Genera configuraciÃ³n de DeepStream

**Clases:**
- `SystemInfo` - Detecta hardware
- `YOLOExporter` - Exporta modelos
- `EngineBuilder` - Construye engines
- `DeepStreamConfig` - Genera configuraciÃ³n

### 2. **entrypoint.sh** - InicializaciÃ³n Docker

```bash
entrypoint.sh
```

**Flujo:**
1. Configura variables de entorno DeepStream
2. Verifica si existe engine TensorRT
3. Si no existe, ejecuta auto_build_engine.py
4. Verifica hardware
5. Ejecuta aplicaciÃ³n o abre terminal

### 3. **Dockerfile** - Contenedor Optimizado

```dockerfile
Dockerfile
```

**Base:** `nvcr.io/nvidia/deepstream:8.0-devel`

**Incluye:**
- CUDA 12.x
- TensorRT
- DeepStream 8.0
- Ultralytics YOLO
- pyservicemaker

## ğŸ”„ Flujo de EjecuciÃ³n

### Primera EjecuciÃ³n (Sin Engine)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usuario ejecuta: docker run -it --gpus all deepstream-app â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /app/entrypoint.sh carga                                   â”‚
â”‚ â€¢ source setup_deepstream_env.sh                           â”‚
â”‚ â€¢ Verifica /app/engines/tensorrt/yolo11x_b1.engine         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
        Â¿Existe Engine?
             â”‚
         NO  â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ auto_build_engine.py inicia                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. DETECTAR HARDWARE                                        â”‚
â”‚    â”œâ”€ nvidia-smi â†’ GPU model, memory                       â”‚
â”‚    â”œâ”€ nvcc â†’ CUDA version                                  â”‚
â”‚    â”œâ”€ tensorrt â†’ TensorRT version                          â”‚
â”‚    â””â”€ /opt/nvidia/deepstream â†’ DeepStream version         â”‚
â”‚                                                             â”‚
â”‚ 2. BUSCAR/GENERAR ONNX                                     â”‚
â”‚    â”œâ”€ Busca yolo11x.onnx en rutas estÃ¡ndar               â”‚
â”‚    â””â”€ Si no existe, descarga yolo11x.pt y lo exporta      â”‚
â”‚                                                             â”‚
â”‚ 3. CONSTRUIR ENGINE TENSORRT                               â”‚
â”‚    â”œâ”€ Parser ONNX â†’ Network TensorRT                      â”‚
â”‚    â”œâ”€ Config batch dinÃ¡mico (1-4-16)                      â”‚
â”‚    â”œâ”€ Habilitar FP16 si disponible                        â”‚
â”‚    â””â”€ Guardar en /app/engines/tensorrt/                   â”‚
â”‚                                                             â”‚
â”‚ 4. GENERAR CONFIG DEEPSTREAM                               â”‚
â”‚    â””â”€ Crear config_infer_auto_generated.txt               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Volver a entrypoint.sh                                      â”‚
â”‚ â€¢ Verificar engine generado                                â”‚
â”‚ â€¢ Imprimir informaciÃ³n de sistema                          â”‚
â”‚ â€¢ Ejecutar aplicaciÃ³n (main_low_latency.py)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejecuciones Posteriores (Con Engine)

```
docker run â†’ entrypoint.sh â†’ Verificar engine â†’ EXISTE â†’ main.py
```

## ğŸ“Š DetecciÃ³n de Hardware

### GPU

```bash
$ nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
NVIDIA RTX 3060, 12288 MB
```

**InformaciÃ³n Capturada:**
- Modelo de GPU
- Memoria disponible
- Cantidad de GPUs

### CUDA

```bash
$ nvcc --version | grep release
release 12.2, V12.2.140
```

**InformaciÃ³n Capturada:**
- VersiÃ³n de CUDA

### TensorRT

```python
import tensorrt as trt
print(trt.__version__)  # 8.6.1
```

**InformaciÃ³n Capturada:**
- VersiÃ³n de TensorRT
- Soporte de FP16

### DeepStream

```bash
$ cat /opt/nvidia/deepstream/deepstream-8.0/version
Version: 8.0.0
```

**InformaciÃ³n Capturada:**
- VersiÃ³n de DeepStream
- UbicaciÃ³n de libraries

## ğŸ”¨ ConstrucciÃ³n del Engine

### ConfiguraciÃ³n de Batch DinÃ¡mico

```
Min shape:  (1, 3, 1280, 1280)   â† 1 cÃ¡mara
Opt shape:  (4, 3, 1280, 1280)   â† 4 cÃ¡maras (optimizado)
Max shape:  (16, 3, 1280, 1280)  â† 16 cÃ¡maras mÃ¡ximo
```

**Ventajas:**
- Un solo engine para 1-16 cÃ¡maras
- TensorRT optimiza para 1, 4 y 16
- Sin necesidad de recompilar

### PrecisiÃ³n

```python
if builder.platform_has_fast_fp16:
    config.set_flag(trt.BuilderFlag.FP16)  # MÃ¡s rÃ¡pido, menos preciso
else:
    # Usa FP32 por defecto
```

**DecisiÃ³n AutomÃ¡tica:**
- FP16 si la GPU lo soporta (2x mÃ¡s rÃ¡pido)
- FP32 como fallback

### Workspace

```python
config.set_memory_pool_limit(
    trt.MemoryPoolType.WORKSPACE,
    8192 * (1 << 20)  # 8 GB
)
```

**Ajuste AutomÃ¡tico:**
- Por defecto: 8192 MB
- Reducible con `--workspace 4096` si hay problemas de memoria

## ğŸ“‹ ConfiguraciÃ³n de DeepStream Generada

```ini
[property]
gpu-id=0
model-engine-file=/app/engines/tensorrt/yolo11x_b1.engine
batch-size=1
infer-dims=3;1280;1280
network-mode=0  # FP32
num-detected-classes=80

[class-attrs-0]
pre-cluster-threshold=0.25
topk=300
```

**Generada AutomÃ¡ticamente:**
- Ruta correcta del engine
- ConfiguraciÃ³n de batch
- Dimensiones del modelo

## ğŸš€ Flujo de Docker a Host

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Host                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ $ docker build -t deepstream-yolo11x .                 â”‚
â”‚ $ docker run -it --gpus all deepstream-yolo11x         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contenedor                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ NVIDIA_VISIBLE_DEVICES=all                           â”‚
â”‚ â€¢ /dev/nvidia0 (GPU fÃ­sica)                            â”‚
â”‚ â€¢ /app/engines/tensorrt (volumen)                      â”‚
â”‚                                                         â”‚
â”‚ âœ… Puede acceder a GPU del host                        â”‚
â”‚ âœ… Genera engine optimizado para esa GPU              â”‚
â”‚ âœ… Guarda engine en volumen persistente               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Ventajas de la Arquitectura

| Aspecto | Sin Auto-Build | Con Auto-Build |
|---------|---|---|
| **Portabilidad** | âŒ Engine GPU-especÃ­fico | âœ… Genera para cada GPU |
| **Setup** | âš ï¸ Manual 30+ minutos | âœ… AutomÃ¡tico 5 min |
| **Compatibilidad** | âŒ RTX 3060 â‰  RTX 4090 | âœ… Compatible con cualquiera |
| **DistribuciÃ³n** | âŒ MÃºltiples engines | âœ… Un Dockerfile |
| **Mantenimiento** | âš ï¸ Actualizar cada GPU | âœ… Auto-actualizable |

## ğŸ¯ Casos de Uso

### Caso 1: Desarrollo Local
```bash
cd /app/export_dynamic_batch
python3 auto_build_engine.py --pt yolo11x.pt
```
âœ… RÃ¡pido, sin Docker

### Caso 2: ProducciÃ³n en GPU Desconocida
```bash
docker run -it --gpus all deepstream-yolo11x
```
âœ… AutomÃ¡tico, seguro

### Caso 3: MÃºltiples Servidores
```bash
# Server 1: GPU A
docker run -it --gpus all deepstream-yolo11x
# Server 2: GPU B (diferente)
docker run -it --gpus all deepstream-yolo11x
```
âœ… Cada uno genera su engine

## ğŸ” Monitoreo y Debugging

### Ver GPU Detectada
```bash
docker exec <container> nvidia-smi
```

### Ver Engine Generado
```bash
docker exec <container> ls -lh /app/engines/tensorrt/
```

### Ver Logs de ConstrucciÃ³n
```bash
docker logs <container> | grep "TensorRT"
```

### Reconstruir Engine
```bash
docker exec <container> python3 auto_build_engine.py --workspace 4096
```

## ğŸ“ˆ Rendimiento Esperado

**Tiempos de ConstrucciÃ³n (Primera EjecuciÃ³n):**

| GPU | FP32 | FP16 |
|-----|------|------|
| RTX 3060 | 8-12 min | 5-8 min |
| RTX 4060 | 6-10 min | 4-6 min |
| RTX 4090 | 3-5 min | 2-3 min |
| A100 | 2-3 min | 1-2 min |

**Inferencia (Por Frame):**

| # CÃ¡maras | RTX 3060 | RTX 4060 | RTX 4090 |
|-----------|----------|----------|----------|
| 1 | 50-60 FPS | 60-70 FPS | 90-100 FPS |
| 4 | 45-50 FPS | 55-65 FPS | 80-90 FPS |
| 8 | 35-40 FPS | 45-55 FPS | 70-80 FPS |

## âš ï¸ Limitaciones

1. **Engine no es portable** - Debe regenerarse en cada GPU
2. **CUDA debe estar disponible** - Se requiere nvidia-docker
3. **Primera ejecuciÃ³n es lenta** - Tiempo de compilaciÃ³n normal

## ğŸ”„ Actualizar Modelo

Para usar un modelo diferente:

```bash
# OpciÃ³n 1: Con ONNX existente
docker run -it --gpus all deepstream-yolo11x \
    python3 auto_build_engine.py --onnx /path/to/custom.onnx

# OpciÃ³n 2: Con PT
docker run -it --gpus all deepstream-yolo11x \
    python3 auto_build_engine.py --pt /path/to/custom.pt
```

---

**Arquitectura DiseÃ±ada Para:** MÃ¡xima flexibilidad y portabilidad
**Fecha:** Noviembre 2025
**DeepStream:** 8.0.0
