# ğŸ“¥ Auto-descarga y ConfiguraciÃ³n de Modelos

## Resumen

El script `auto_build_engine.py` ahora **descarga automÃ¡ticamente** el modelo YOLO11x y genera el engine TensorRT sin necesidad de subir archivos grandes a GitHub.

## Flujo AutomÃ¡tico

```
auto_build_engine.py ejecutado
    â†“
Â¿Existe ONNX local?
    â”œâ†’ SÃ: Usar directamente
    â””â†’ NO: Â¿Existe PT local?
        â”œâ†’ SÃ: Convertir a ONNX
        â””â†’ NO: Descargar yolo11x.pt automÃ¡ticamente
            â†“
        Convertir PT â†’ ONNX
            â†“
        Generar TensorRT engine
            â†“
        Crear configuraciÃ³n DeepStream
```

## Ventajas

âœ… **No necesitas subir modelos grandes a GitHub**
- ONNX (219 MB) y PT (257 MB) se excluyen del repositorio
- Repositorio mÃ¡s limpio y eficiente

âœ… **Funciona en cualquier PC**
- Auto-descarga el modelo la primera vez
- Genera engine optimizado para tu GPU especÃ­fica

âœ… **100% automatizado**
- Solo ejecuta: `python3 auto_build_engine.py`
- Todo se configura solo

## Uso

### OpciÃ³n 1: Auto-detecciÃ³n (Recomendado)

```bash
python3 /app/engines/auto_build_engine.py
```

El script:
1. Busca ONNX local
2. Si no existe, busca PT local
3. Si no existe, descarga yolo11x.pt automÃ¡ticamente
4. Convierte a ONNX
5. Genera engine TensorRT
6. Crea configuraciÃ³n DeepStream

### OpciÃ³n 2: Usar ONNX local

```bash
python3 /app/engines/auto_build_engine.py --onnx /ruta/a/tu/modelo.onnx
```

### OpciÃ³n 3: Usar PT local

```bash
python3 /app/engines/auto_build_engine.py --pt /ruta/a/tu/modelo.pt
```

### OpciÃ³n 4: Especificar salida

```bash
python3 /app/engines/auto_build_engine.py \
    --output /app/engines/tensorrt/yolo11x_custom.engine \
    --workspace 16384
```

## ParÃ¡metros

| ParÃ¡metro | DescripciÃ³n | Defecto |
|-----------|-------------|---------|
| `--onnx` | Ruta a modelo ONNX | Auto-detectar |
| `--pt` | Ruta a modelo PT | Auto-detectar |
| `--output` | Ruta salida engine | `*.engine` mismo directorio |
| `--workspace` | Memoria workspace (MB) | 8192 |
| `--no-fp16` | Desabilitar FP16 | FP16 habilitado |

## Descarga AutomÃ¡tica

### Primera EjecuciÃ³n

La primera vez tarda mÃ¡s tiempo (10-15 minutos):

```
ğŸ“¥ DESCARGANDO MODELO YOLO11x
=======================================================================
ğŸ“‚ Directorio de destino: /app/engines/pt
â³ Descargando modelo (5-10 minutos)...
   TamaÃ±o: ~219 MB

... [descargando] ...

âœ… Modelo descargado: /app/engines/pt/yolo11x.pt
ğŸ“Š TamaÃ±o: 257.00 MB

ğŸ“¦ EXPORTANDO MODELO YOLO PT A ONNX
=======================================================================
ğŸ“‚ Cargando modelo: /app/engines/pt/yolo11x.pt

âš™ï¸  ConfiguraciÃ³n de exportaciÃ³n:
   Formato: ONNX
   TamaÃ±o entrada: 1280x1280
   Batch: DINÃMICO (1-16)
   Opset: 17

ğŸ”„ Exportando (2-5 minutos)...

âœ… ONNX exportado: /app/engines/onnx/yolo11x.onnx
```

### Ejecuciones Posteriores

Si ya existe ONNX o PT localmente, se salta la descarga:

```
âœ… Modelo ONNX ya existe: /app/engines/onnx/yolo11x.onnx
   TamaÃ±o: 219.34 MB

ğŸš€ GENERANDO ENGINE TENSORRT
=======================================================================
... [genera engine] ...
```

## Directorios

DespuÃ©s de la primera ejecuciÃ³n:

```
/app/engines/
â”œâ”€â”€ auto_build_engine.py      # Script principal
â”œâ”€â”€ pt/
â”‚   â””â”€â”€ yolo11x.pt           # Descargado automÃ¡ticamente
â”œâ”€â”€ onnx/
â”‚   â””â”€â”€ yolo11x.onnx         # Generado automÃ¡ticamente
â””â”€â”€ tensorrt/
    â””â”€â”€ yolo11x_b1.engine    # Engine compilado
```

## GitHub

âœ… **No hay archivos grandes en el repositorio**

```
.gitignore:
*.onnx      # Excluye todos los ONNX
*.pt        # Excluye todos los PT
engines/onnx/
engines/pt/
```

Cuando clonas el repositorio:
- âœ… Obtiene `auto_build_engine.py`
- âœ… Obtiene `entrypoint.sh`
- âœ… Obtiene todo el cÃ³digo
- âŒ No obtiene modelos grandes (se descargan automÃ¡ticamente)

## Docker

El `entrypoint.sh` ejecuta automÃ¡ticamente:

```bash
python3 /app/engines/auto_build_engine.py
```

AsÃ­ que cuando corres:

```bash
docker run -it deepstream-app python3 main_low_latency.py
```

El proceso completo es:
1. Descargar yolo11x.pt (si no existe)
2. Exportar a ONNX (si no existe)
3. Generar engine TensorRT (si no existe)
4. Iniciar aplicaciÃ³n con engine generado

## Troubleshooting

### Error: "No GPU detected"

```bash
# Verificar GPU
nvidia-smi

# Ejecutar con CPU (muy lento, no recomendado)
python3 auto_build_engine.py --no-fp16
```

### Error: "ONNX parse failed"

```bash
# Usar PT en lugar de ONNX
python3 auto_build_engine.py --pt /app/engines/pt/yolo11x.pt
```

### Error: "Out of memory"

```bash
# Reducir workspace
python3 auto_build_engine.py --workspace 4096
```

### Descarga muy lenta

La descarga de 219 MB depende de tu conexiÃ³n:
- ConexiÃ³n 1 Mbps: ~30 minutos
- ConexiÃ³n 10 Mbps: ~3 minutos
- ConexiÃ³n 100 Mbps: ~20 segundos

Usa una conexiÃ³n mÃ¡s rÃ¡pida si es posible.

## ConfiguraciÃ³n DeepStream

El script automÃ¡ticamente crea configuraciÃ³n en:

```
/app/configs/deepstream/config_infer_primary_yolo11x_b1.txt
```

Con la ruta al engine generado listo para usar.

## PrÃ³ximos Pasos

```bash
# 1. (Opcional) Generar engine manualmente
python3 /app/engines/auto_build_engine.py

# 2. Usar en Docker
docker build -t deepstream-app .
docker run -it deepstream-app python3 main_low_latency.py

# 3. O en local
python3 main_low_latency.py
```

