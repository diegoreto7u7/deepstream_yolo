# ğŸš€ Quick Start - DeepStream + YOLO11x con Auto-Build

## DescripciÃ³n

Este proyecto implementa un sistema de detecciÃ³n de personas con conteo de entradas/salidas usando:
- **YOLO11x** para detecciÃ³n
- **DeepStream 8.0** para procesamiento en GPU
- **TensorRT** para inferencia optimizada
- **Auto-generaciÃ³n de engine** optimizado para tu GPU

## ğŸ“‹ Requisitos

### Hardware
- **GPU NVIDIA** (RTX 3060, RTX 4060, A100, etc.)
- **MÃ­nimo 6GB de VRAM GPU**
- **MÃ­nimo 8GB de RAM**
- **Docker con soporte NVIDIA**

### Software
- Docker (versiÃ³n 19.03+)
- NVIDIA Container Toolkit
- nvidia-docker

## ğŸš€ Uso RÃ¡pido

### OpciÃ³n 1: Con Docker (Recomendado)

```bash
# Construir imagen Docker
docker build -t deepstream-yolo11x .

# Ejecutar contenedor
docker run -it --gpus all \
    -e NVIDIA_VISIBLE_DEVICES=all \
    deepstream-yolo11x

# El contenedor detectarÃ¡ tu GPU y generarÃ¡ el engine automÃ¡ticamente
```

### OpciÃ³n 2: En el Host (Sin Docker)

```bash
# 1. Instalar dependencias
pip3 install ultralytics tensorrt pyservicemaker

# 2. Configurar DeepStream
source setup_deepstream_env.sh

# 3. Auto-generar engine
cd export_dynamic_batch
python3 auto_build_engine.py

# 4. Ejecutar aplicaciÃ³n
cd ..
python3 main_low_latency.py
```

## ğŸ”§ GeneraciÃ³n AutomÃ¡tica de Engine

El sistema detecta automÃ¡ticamente:
- âœ… GPU disponible (modelo, memoria)
- âœ… VersiÃ³n de CUDA
- âœ… VersiÃ³n de TensorRT
- âœ… DeepStream instalado

### Uso Manual del Script

```bash
# Auto-detectar todo y usar yolo11x.onnx por defecto
python3 auto_build_engine.py

# Usar un ONNX especÃ­fico
python3 auto_build_engine.py --onnx /ruta/a/model.onnx

# Exportar de PT a ONNX y luego a engine
python3 auto_build_engine.py --pt /ruta/a/model.pt

# Con opciones personalizadas
python3 auto_build_engine.py \
    --onnx model.onnx \
    --workspace 4096 \
    --no-fp16 \
    --output custom_engine.engine
```

## ğŸ“Š Estructura del Proyecto

```
/app/
â”œâ”€â”€ export_dynamic_batch/
â”‚   â”œâ”€â”€ auto_build_engine.py          # Script principal de auto-build
â”‚   â”œâ”€â”€ yolo11x.onnx                  # Modelo ONNX (si existe)
â”‚   â””â”€â”€ yolo11x.pt                    # Modelo PT (descargado)
â”‚
â”œâ”€â”€ engines/
â”‚   â”œâ”€â”€ tensorrt/
â”‚   â”‚   â””â”€â”€ yolo11x_b1.engine         # Engine generado automÃ¡ticamente
â”‚   â””â”€â”€ onnx/
â”‚       â””â”€â”€ yolo11x_dynamic.onnx
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ deepstream/
â”‚       â”œâ”€â”€ config_infer_*.txt        # Configuraciones de DeepStream
â”‚       â””â”€â”€ labels.txt
â”‚
â”œâ”€â”€ deepstream_api/
â”‚   â”œâ”€â”€ main_low_latency.py           # Entrada principal (baja latencia)
â”‚   â”œâ”€â”€ main.py                       # Entrada normal
â”‚   â”œâ”€â”€ main_headless.py              # Sin interfaz grÃ¡fica
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ deepstream_camera_sm.py
â”‚       â”œâ”€â”€ line_crossing_detector.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ entrypoint.sh                     # Script de inicializaciÃ³n Docker
â”œâ”€â”€ Dockerfile                        # Dockerfile optimizado
â”œâ”€â”€ setup_deepstream_env.sh           # ConfiguraciÃ³n de variables de entorno
â””â”€â”€ QUICKSTART.md                     # Este archivo
```

## ğŸ¯ Flujo de EjecuciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker run               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ entrypoint.sh            â”‚
â”‚  â€¢ Load DeepStream env   â”‚
â”‚  â€¢ Check GPU             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    Â¿Engine existe?
         /  \
       SÃ    NO
       â”‚      â”‚
       â”‚      â–¼
       â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   â”‚ auto_build_engine.py â”‚
       â”‚   â”‚  â€¢ Detect GPU        â”‚
       â”‚   â”‚  â€¢ Build engine      â”‚
       â”‚   â”‚  â€¢ Save to /engines/ â”‚
       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main_low_latency.py      â”‚
â”‚  â€¢ Load API cÃ¡maras      â”‚
â”‚  â€¢ Setup pipelines       â”‚
â”‚  â€¢ Run inference         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” VerificaciÃ³n del Sistema

```bash
# Verificar GPU
nvidia-smi

# Verificar CUDA
nvcc --version

# Verificar TensorRT
python3 -c "import tensorrt; print(tensorrt.__version__)"

# Verificar DeepStream
cat /opt/nvidia/deepstream/deepstream-8.0/version

# Verificar engine
ls -lh /app/engines/tensorrt/*.engine
```

## âš ï¸ SoluciÃ³n de Problemas

### "GPU no detectada"
```bash
# Verificar NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:12.0-runtime-ubuntu20.04 nvidia-smi
```

### "TensorRT no disponible"
```bash
# Instalar dentro del contenedor
pip3 install tensorrt
```

### "CUDA out of memory"
```bash
# Reducir workspace
python3 auto_build_engine.py --workspace 4096
```

### "FP16 no disponible"
```bash
# Usar FP32
python3 auto_build_engine.py --no-fp16
```

## ğŸ“ ConfiguraciÃ³n de CÃ¡maras

Las cÃ¡maras se configuran a travÃ©s de una API Laravel. Edita `deepstream_api/main_low_latency.py`:

```python
api_url = "http://tu-laravel-api.com/api"
```

Las lÃ­neas de cruce se definen en formato:
```json
{
  "cam_coordenadas": {
    "start": [960, 540],
    "end": [1200, 540],
    "direccion_entrada": "izquierda"
  }
}
```

## ğŸš€ Ejecutar la AplicaciÃ³n

### Modo Baja Latencia (Recomendado)
```bash
python3 main_low_latency.py
```

### Modo Normal
```bash
python3 main.py
```

### Modo Headless (Solo terminal, sin display)
```bash
python3 main_headless.py
```

## ğŸ“Š Esperado en ProducciÃ³n

Con RTX 3060 / RTX 4060:
- **1 cÃ¡mara**: ~50-60 FPS
- **4 cÃ¡maras**: ~45-50 FPS cada una
- **8+ cÃ¡maras**: Rendimiento degradado

## ğŸ” Notas de Seguridad

- El engine TensorRT es especÃ­fico para cada GPU
- Cada contenedor genera su propio engine automÃ¡ticamente
- Los engines no son portables entre diferentes GPUs

## ğŸ“š DocumentaciÃ³n Adicional

- [DeepStream Official Docs](https://docs.nvidia.com/metropolis/deepstream/dev-guide/)
- [YOLO11 Documentation](https://docs.ultralytics.com/)
- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)

## âœ… Checklist de Inicio

- [ ] Docker instalado con soporte NVIDIA
- [ ] Imagen construida: `docker build -t deepstream-yolo11x .`
- [ ] Contenedor ejecutÃ¡ndose: `docker run -it --gpus all deepstream-yolo11x`
- [ ] Engine generado automÃ¡ticamente
- [ ] API Laravel configurada
- [ ] CÃ¡maras conectadas
- [ ] LÃ­neas de cruce definidas

## ğŸ¤ Soporte

Para problemas, revisar los logs:
```bash
docker logs <container-id>
```

O ejecutar en el contenedor:
```bash
python3 export_dynamic_batch/auto_build_engine.py
```

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025
**DeepStream**: 8.0.0
**YOLO**: 11x
